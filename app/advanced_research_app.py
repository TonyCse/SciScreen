"""Application Streamlit ultra-avancÃ©e pour la recherche bibliographique.

FonctionnalitÃ©s :
1. Vraies APIs scientifiques (OpenAlex, Semantic Scholar, PubMed, Crossref)
2. Import/customisation/export Excel avancÃ©
3. Interface d'Ã©dition interactive
4. Suppression/modification de lignes
5. Export personnalisÃ©
"""

import sys
from pathlib import Path
import pandas as pd
import streamlit as st
from datetime import datetime
import time
import io
import requests
import requests.exceptions
import json
from typing import List, Dict, Any
import xml.etree.ElementTree as ET
from urllib.parse import quote

# Ajouter le rÃ©pertoire src au chemin Python
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

# Configuration des APIs
API_CONFIG = {
    "openalex": {
        "base_url": "https://api.openalex.org",
        "rate_limit": 10,  # requÃªtes par seconde
        "free": True
    },
    "semantic_scholar": {
        "base_url": "https://api.semanticscholar.org/graph/v1",
        "rate_limit": 100,  # requÃªtes par minute
        "free": True
    },
    "pubmed": {
        "base_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils",
        "rate_limit": 3,  # requÃªtes par seconde
        "free": True
    },
    "crossref": {
        "base_url": "https://api.crossref.org",
        "rate_limit": 50,  # requÃªtes par seconde avec politeness
        "free": True
    }
}

class OpenAlexAPI:
    """Client pour l'API OpenAlex."""
    
    def __init__(self):
        self.base_url = API_CONFIG["openalex"]["base_url"]
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'LitReviewPipeline/1.0 (https://github.com/user/lit-review-pipeline)',
            'Accept': 'application/json'
        })
    
    def search_works(self, query: str, year_from: int, year_to: int, max_results: int = 100) -> pd.DataFrame:
        """Rechercher des articles sur OpenAlex."""
        try:
            params = {
                'search': query,
                'filter': f'publication_year:{year_from}-{year_to}',
                'per-page': min(max_results, 200),
                'cursor': '*',
                'sort': 'cited_by_count:desc'
            }
            
            response = self.session.get(f"{self.base_url}/works", params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            articles = []
            
            if not data or 'results' not in data:
                return pd.DataFrame()
            
            for work in data.get('results', []):
                if not work or not isinstance(work, dict):
                    continue
                    
                # Extraction sÃ©curisÃ©e des donnÃ©es
                primary_location = work.get('primary_location') or {}
                source_info = primary_location.get('source') or {}
                
                article = {
                    'source': 'OpenAlex',
                    'title': work.get('title') or 'Sans titre',
                    'abstract': (work.get('abstract_inverted_index') and 'RÃ©sumÃ© disponible') or work.get('abstract') or '',
                    'authors': self._format_authors(work.get('authorships') or []),
                    'journal': source_info.get('display_name') or 'Journal non spÃ©cifiÃ©',
                    'year': work.get('publication_year') or 0,
                    'doi': (work.get('doi') or '').replace('https://doi.org/', ''),
                    'cited_by': work.get('cited_by_count') or 0,
                    'url': work.get('id') or '',
                    'type': work.get('type') or 'article',
                    'open_access': (work.get('open_access') or {}).get('is_oa', False)
                }
                articles.append(article)
            
            return pd.DataFrame(articles)
            
        except Exception as e:
            st.error(f"Erreur OpenAlex: {e}")
            return pd.DataFrame()
    
    def _format_authors(self, authorships: List[Dict]) -> str:
        """Formater la liste des auteurs."""
        authors = []
        for auth in authorships[:5]:  # Limiter Ã  5 auteurs
            author = auth.get('author', {})
            name = author.get('display_name', '')
            if name:
                authors.append(name)
        
        result = '; '.join(authors)
        if len(authorships) > 5:
            result += ' et al.'
        return result

class SemanticScholarAPI:
    """Client pour l'API Semantic Scholar."""
    
    def __init__(self):
        self.base_url = API_CONFIG["semantic_scholar"]["base_url"]
        self.session = requests.Session()
        self.session.headers.update({
            'Accept': 'application/json'
        })
    
    def search_papers(self, query: str, year_from: int, year_to: int, max_results: int = 100) -> pd.DataFrame:
        """Rechercher des articles sur Semantic Scholar."""
        try:
            # Limiter les requÃªtes pour Ã©viter les erreurs 429
            max_results = min(max_results, 50)
            
            params = {
                'query': query,
                'year': f'{year_from}-{year_to}',
                'limit': max_results,
                'fields': 'paperId,title,abstract,authors,journal,year,citationCount,url,venue,publicationTypes,isOpenAccess'
            }
            
            # DÃ©lai pour respecter les limites
            time.sleep(1)
            
            response = self.session.get(f"{self.base_url}/paper/search", params=params, timeout=30)
            
            if response.status_code == 429:
                st.warning("âš ï¸ Semantic Scholar: Limite de taux atteinte, attente...")
                time.sleep(10)
                response = self.session.get(f"{self.base_url}/paper/search", params=params, timeout=30)
            
            response.raise_for_status()
            
            data = response.json()
            articles = []
            
            if not data or 'data' not in data:
                return pd.DataFrame()
            
            for paper in data.get('data', []):
                if not paper or not isinstance(paper, dict):
                    continue
                
                journal_info = paper.get('journal') or {}
                
                article = {
                    'source': 'Semantic Scholar',
                    'title': paper.get('title') or 'Sans titre',
                    'abstract': paper.get('abstract') or '',
                    'authors': self._format_authors(paper.get('authors') or []),
                    'journal': journal_info.get('name') or paper.get('venue') or 'Journal non spÃ©cifiÃ©',
                    'year': paper.get('year') or 0,
                    'doi': '',  # Semantic Scholar ne fournit pas directement le DOI
                    'cited_by': paper.get('citationCount') or 0,
                    'url': paper.get('url') or '',
                    'type': ', '.join(paper.get('publicationTypes') or []) or 'article',
                    'open_access': paper.get('isOpenAccess', False)
                }
                articles.append(article)
            
            return pd.DataFrame(articles)
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                st.warning("âš ï¸ Semantic Scholar: Trop de requÃªtes, rÃ©essayez dans quelques minutes")
            else:
                st.error(f"Erreur HTTP Semantic Scholar: {e}")
            return pd.DataFrame()
        except Exception as e:
            st.error(f"Erreur Semantic Scholar: {e}")
            return pd.DataFrame()
    
    def _format_authors(self, authors: List[Dict]) -> str:
        """Formater la liste des auteurs."""
        author_names = []
        for author in authors[:5]:  # Limiter Ã  5 auteurs
            name = author.get('name', '')
            if name:
                author_names.append(name)
        
        result = '; '.join(author_names)
        if len(authors) > 5:
            result += ' et al.'
        return result

class PubMedAPI:
    """Client pour l'API PubMed E-utilities."""
    
    def __init__(self):
        self.base_url = API_CONFIG["pubmed"]["base_url"]
        self.session = requests.Session()
    
    def search_articles(self, query: str, year_from: int, year_to: int, max_results: int = 100) -> pd.DataFrame:
        """Rechercher des articles sur PubMed."""
        try:
            # Ã‰tape 1: Recherche des IDs
            search_params = {
                'db': 'pubmed',
                'term': f'{query} AND {year_from}[PDAT]:{year_to}[PDAT]',
                'retmax': min(max_results, 200),
                'retmode': 'json',
                'sort': 'relevance'
            }
            
            search_response = self.session.get(f"{self.base_url}/esearch.fcgi", params=search_params)
            search_response.raise_for_status()
            search_data = search_response.json()
            
            pmids = search_data.get('esearchresult', {}).get('idlist', [])
            
            if not pmids:
                return pd.DataFrame()
            
            # Ã‰tape 2: RÃ©cupÃ©ration des dÃ©tails
            fetch_params = {
                'db': 'pubmed',
                'id': ','.join(pmids),
                'retmode': 'xml'
            }
            
            fetch_response = self.session.get(f"{self.base_url}/efetch.fcgi", params=fetch_params)
            fetch_response.raise_for_status()
            
            articles = self._parse_pubmed_xml(fetch_response.text)
            return pd.DataFrame(articles)
            
        except Exception as e:
            st.error(f"Erreur PubMed: {e}")
            return pd.DataFrame()
    
    def _parse_pubmed_xml(self, xml_text: str) -> List[Dict]:
        """Parser le XML de PubMed."""
        articles = []
        try:
            root = ET.fromstring(xml_text)
            
            for article_elem in root.findall('.//PubmedArticle'):
                article_data = article_elem.find('.//Article')
                if article_data is None:
                    continue
                
                # Titre
                title_elem = article_data.find('.//ArticleTitle')
                title = title_elem.text if title_elem is not None else ''
                
                # RÃ©sumÃ©
                abstract_elem = article_data.find('.//Abstract/AbstractText')
                abstract = abstract_elem.text if abstract_elem is not None else ''
                
                # Auteurs
                authors = []
                for author_elem in article_data.findall('.//AuthorList/Author'):
                    lastname = author_elem.find('.//LastName')
                    forename = author_elem.find('.//ForeName')
                    if lastname is not None:
                        name = lastname.text
                        if forename is not None:
                            name = f"{name}, {forename.text}"
                        authors.append(name)
                
                # Journal
                journal_elem = article_data.find('.//Journal/Title')
                journal = journal_elem.text if journal_elem is not None else ''
                
                # AnnÃ©e
                year_elem = article_data.find('.//PubDate/Year')
                year = int(year_elem.text) if year_elem is not None else None
                
                # PMID
                pmid_elem = article_elem.find('.//PMID')
                pmid = pmid_elem.text if pmid_elem is not None else ''
                
                # DOI
                doi = ''
                for doi_elem in article_data.findall('.//ELocationID'):
                    if doi_elem.get('EIdType') == 'doi':
                        doi = doi_elem.text
                        break
                
                article = {
                    'source': 'PubMed',
                    'title': title,
                    'abstract': abstract,
                    'authors': '; '.join(authors[:5]) + (' et al.' if len(authors) > 5 else ''),
                    'journal': journal,
                    'year': year,
                    'doi': doi,
                    'cited_by': 0,  # PubMed ne fournit pas directement le nombre de citations
                    'url': f'https://pubmed.ncbi.nlm.nih.gov/{pmid}/' if pmid else '',
                    'pmid': pmid,
                    'type': 'Article'
                }
                articles.append(article)
                
        except ET.ParseError as e:
            st.error(f"Erreur parsing XML PubMed: {e}")
        
        return articles

class CrossrefAPI:
    """Client pour l'API Crossref."""
    
    def __init__(self):
        self.base_url = API_CONFIG["crossref"]["base_url"]
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'LitReviewPipeline/1.0 (mailto:user@example.com)'
        })
    
    def search_works(self, query: str, year_from: int, year_to: int, max_results: int = 100) -> pd.DataFrame:
        """Rechercher des articles sur Crossref."""
        try:
            params = {
                'query': query,
                'filter': f'from-pub-date:{year_from},until-pub-date:{year_to}',
                'rows': min(max_results, 200),
                'sort': 'relevance',
                'order': 'desc'
            }
            
            response = self.session.get(f"{self.base_url}/works", params=params)
            response.raise_for_status()
            
            data = response.json()
            articles = []
            
            for work in data.get('message', {}).get('items', []):
                # Date de publication
                pub_date = work.get('published-print', work.get('published-online', {}))
                year = None
                if pub_date and 'date-parts' in pub_date:
                    year = pub_date['date-parts'][0][0] if pub_date['date-parts'][0] else None
                
                article = {
                    'source': 'Crossref',
                    'title': ' '.join(work.get('title', [])),
                    'abstract': work.get('abstract', ''),
                    'authors': self._format_authors(work.get('author', [])),
                    'journal': work.get('container-title', [''])[0],
                    'year': year,
                    'doi': work.get('DOI', ''),
                    'cited_by': work.get('is-referenced-by-count', 0),
                    'url': work.get('URL', ''),
                    'type': work.get('type', ''),
                    'open_access': 'license' in work
                }
                articles.append(article)
            
            return pd.DataFrame(articles)
            
        except Exception as e:
            st.error(f"Erreur Crossref: {e}")
            return pd.DataFrame()
    
    def _format_authors(self, authors: List[Dict]) -> str:
        """Formater la liste des auteurs."""
        author_names = []
        for author in authors[:5]:  # Limiter Ã  5 auteurs
            given = author.get('given', '')
            family = author.get('family', '')
            if family:
                name = f"{family}, {given}" if given else family
                author_names.append(name)
        
        result = '; '.join(author_names)
        if len(authors) > 5:
            result += ' et al.'
        return result

def create_extended_columns():
    """CrÃ©er les colonnes Ã©tendues pour l'analyse bibliographique."""
    return {
        'ABS 1 OU 0': '',
        'Notes': '', 
        'Type revue': '',
        'WL = mesure': '',
        'Chr = participants': '',
        'SpÃ©cialitÃ©': '',
        'Intervention': '',
        'Technique': '',
        'Contexte': '',
        'Simulation ?': '',
        'Outils': '',
        'RÃ©sultats ?': '',
        'Additional outcomes / measures': '',
        'Exclusion': ''
    }

def detect_review_type(title):
    """DÃ©tecter le type de revue Ã  partir du titre."""
    if pd.isna(title):
        return ''
    
    title_lower = str(title).lower()
    
    if 'systematic review' in title_lower or 'systematic literature review' in title_lower:
        return 'Revue systÃ©matique'
    elif 'meta-analysis' in title_lower or 'meta analysis' in title_lower:
        return 'MÃ©ta-analyse'
    elif 'scoping review' in title_lower:
        return 'Scoping review'
    elif 'narrative review' in title_lower:
        return 'Revue narrative'
    elif 'randomized controlled trial' in title_lower or 'rct' in title_lower:
        return 'ECR'
    elif 'cohort study' in title_lower:
        return 'Ã‰tude de cohorte'
    elif 'case study' in title_lower or 'case report' in title_lower:
        return 'Ã‰tude de cas'
    elif 'cross-sectional' in title_lower:
        return 'Ã‰tude transversale'
    else:
        return ''

def detect_specialty(journal):
    """DÃ©tecter la spÃ©cialitÃ© mÃ©dicale Ã  partir du nom du journal."""
    if pd.isna(journal):
        return ''
    
    journal_lower = str(journal).lower()
    
    # Dictionnaire de spÃ©cialitÃ©s basÃ© sur les mots-clÃ©s du journal
    specialties = {
        'psychiatry': 'Psychiatrie',
        'psychology': 'Psychologie',
        'mental health': 'SantÃ© mentale',
        'surgery': 'Chirurgie',
        'anesthesi': 'AnesthÃ©sie',
        'cardiology': 'Cardiologie',
        'neurology': 'Neurologie',
        'oncology': 'Oncologie',
        'pediatric': 'PÃ©diatrie',
        'emergency': 'Urgences',
        'intensive care': 'Soins intensifs',
        'radiology': 'Radiologie',
        'orthopedic': 'OrthopÃ©die',
        'dermatology': 'Dermatologie',
        'ophthalmology': 'Ophtalmologie',
        'otolaryngology': 'ORL',
        'urology': 'Urologie',
        'gynecology': 'GynÃ©cologie',
        'nursing': 'Soins infirmiers',
        'rehabilitation': 'RÃ©Ã©ducation',
        'pharmacology': 'Pharmacologie'
    }
    
    for keyword, specialty in specialties.items():
        if keyword in journal_lower:
            return specialty
    
    return ''

def main():
    """Application principale."""
    st.set_page_config(
        page_title="Recherche Bibliographique AvancÃ©e",
        page_icon="ðŸ”¬",
        layout="wide"
    )
    
    st.title("ðŸ”¬ Recherche Bibliographique AvancÃ©e")
    st.markdown("**Vraies APIs scientifiques + Customisation Excel complÃ¨te**")
    
    # Tabs pour les diffÃ©rentes fonctionnalitÃ©s
    tab1, tab2, tab3 = st.tabs([
        "ðŸ”Ž Recherche Multi-API", 
        "ðŸ“Š Import & Customisation Excel",
        "âš™ï¸ Configuration APIs"
    ])
    
    with tab1:
        st.header("ðŸ”Ž Recherche avec APIs RÃ©elles")
        recherche_multi_api()
    
    with tab2:
        st.header("ðŸ“Š Customisation Excel AvancÃ©e")
        customisation_excel()
    
    with tab3:
        st.header("âš™ï¸ Configuration des APIs")
        configuration_apis()

def recherche_multi_api():
    """Interface pour la recherche avec vraies APIs."""
    
    # Configuration dans la sidebar
    st.sidebar.header("ðŸ”¬ Configuration Recherche")
    
    # ParamÃ¨tres de recherche
    query = st.sidebar.text_area(
        "RequÃªte de recherche",
        value="machine learning healthcare",
        help="Mots-clÃ©s en anglais. Exemples: 'covid vaccine', 'cancer therapy', 'AI diagnosis'"
    )
    
    # Plage d'annÃ©es
    col1, col2 = st.sidebar.columns(2)
    with col1:
        year_from = st.number_input("AnnÃ©e dÃ©but", 2018, 2024, 2022)
    with col2:
        year_to = st.number_input("AnnÃ©e fin", 2018, 2024, 2024)
    
    max_results = st.sidebar.slider("Articles par API", 10, 200, 50)
    
    # SÃ©lection des APIs
    st.sidebar.subheader("ðŸ“š APIs Scientifiques")
    
    use_openalex = st.sidebar.checkbox("ðŸŒ OpenAlex (Gratuit)", value=True, 
                                       help="Base de donnÃ©es bibliographique open source")
    use_semantic = st.sidebar.checkbox("ðŸ§  Semantic Scholar (Gratuit)", value=True,
                                       help="Moteur de recherche acadÃ©mique avec IA")
    use_pubmed = st.sidebar.checkbox("ðŸ¥ PubMed (Gratuit)", value=False,
                                     help="Base biomÃ©dicale officielle NCBI")
    use_crossref = st.sidebar.checkbox("ðŸ“„ Crossref (Gratuit)", value=False,
                                       help="MÃ©tadonnÃ©es d'articles acadÃ©miques")
    
    # Bouton de recherche
    if st.sidebar.button("ðŸš€ Lancer Recherche Multi-API", type="primary"):
        if not query.strip():
            st.error("âš ï¸ Veuillez saisir une requÃªte de recherche")
            return
        
        # Affichage des paramÃ¨tres
        st.subheader("ðŸ“‹ ParamÃ¨tres de Recherche")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.info(f"**RequÃªte:** {query}")
        with col2:
            st.info(f"**PÃ©riode:** {year_from}-{year_to}")
        with col3:
            apis_selected = sum([use_openalex, use_semantic, use_pubmed, use_crossref])
            st.info(f"**APIs:** {apis_selected} sÃ©lectionnÃ©es")
        
        # Progress tracking
        all_results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        apis_to_run = []
        if use_openalex: apis_to_run.append(("OpenAlex", OpenAlexAPI))
        if use_semantic: apis_to_run.append(("Semantic Scholar", SemanticScholarAPI))
        if use_pubmed: apis_to_run.append(("PubMed", PubMedAPI))
        if use_crossref: apis_to_run.append(("Crossref", CrossrefAPI))
        
        for i, (api_name, api_class) in enumerate(apis_to_run):
            status_text.text(f"ðŸ” Recherche sur {api_name}...")
            progress = (i + 0.5) / len(apis_to_run)
            progress_bar.progress(progress)
            
            try:
                api_client = api_class()
                
                if api_name == "OpenAlex":
                    df = api_client.search_works(query, year_from, year_to, max_results)
                elif api_name == "Semantic Scholar":
                    df = api_client.search_papers(query, year_from, year_to, max_results)
                elif api_name == "PubMed":
                    df = api_client.search_articles(query, year_from, year_to, max_results)
                elif api_name == "Crossref":
                    df = api_client.search_works(query, year_from, year_to, max_results)
                
                if not df.empty:
                    all_results.append(df)
                    st.success(f"âœ… {api_name}: {len(df)} articles")
                else:
                    st.warning(f"âš ï¸ {api_name}: Aucun rÃ©sultat")
                
                # DÃ©lai pour respecter les limites de taux
                time.sleep(0.5)
                
            except Exception as e:
                st.error(f"âŒ Erreur {api_name}: {e}")
            
            progress = (i + 1) / len(apis_to_run)
            progress_bar.progress(progress)
        
        # Fusion des rÃ©sultats
        if all_results:
            status_text.text("ðŸ”„ Fusion des rÃ©sultats...")
            combined_df = pd.concat(all_results, ignore_index=True)
            
            # DÃ©duplication basique par titre
            combined_df = combined_df.drop_duplicates(subset=['title'], keep='first')
            
            st.session_state['search_results'] = combined_df
            progress_bar.progress(1.0)
            status_text.text("âœ… Recherche terminÃ©e!")
            
            st.success(f"ðŸŽ‰ **Total: {len(combined_df)} articles uniques trouvÃ©s**")
        else:
            st.error("âŒ Aucun rÃ©sultat trouvÃ© sur les APIs sÃ©lectionnÃ©es")
    
    # Affichage des rÃ©sultats
    if 'search_results' in st.session_state:
        df = st.session_state['search_results']
        
        st.subheader(f"ðŸ“Š RÃ©sultats ({len(df)} articles)")
        
        # Statistiques
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Articles", len(df))
        with col2:
            sources = df['source'].nunique()
            st.metric("Sources", sources)
        with col3:
            with_abstract = (df['abstract'] != "").sum()
            st.metric("Avec rÃ©sumÃ©", with_abstract)
        with col4:
            total_citations = df['cited_by'].sum()
            st.metric("Citations", total_citations)
        
        # Filtres
        col1, col2 = st.columns(2)
        with col1:
            sources_filter = st.multiselect(
                "Filtrer par source",
                options=df['source'].unique(),
                default=df['source'].unique()
            )
        with col2:
            if 'year' in df.columns:
                years = sorted([y for y in df['year'].unique() if pd.notna(y)])
                if years:
                    year_range = st.select_slider(
                        "Plage d'annÃ©es",
                        options=years,
                        value=(min(years), max(years))
                    )
        
        # Application des filtres
        filtered_df = df[df['source'].isin(sources_filter)]
        if 'year_range' in locals() and 'year' in filtered_df.columns:
            filtered_df = filtered_df[
                (filtered_df['year'] >= year_range[0]) & 
                (filtered_df['year'] <= year_range[1])
            ]
        
        # Affichage du tableau
        display_columns = ['title', 'authors', 'journal', 'year', 'source', 'cited_by']
        display_df = filtered_df[display_columns].copy()
        
        # Tronquer les titres longs
        display_df['title'] = display_df['title'].apply(
            lambda x: x[:80] + "..." if len(str(x)) > 80 else x
        )
        
        st.dataframe(display_df, use_container_width=True)
        
        # Export vers customisation
        if st.button("ðŸ“Š Envoyer vers Customisation Excel", type="secondary"):
            st.session_state['excel_data'] = filtered_df
            st.success("âœ… DonnÃ©es envoyÃ©es vers l'onglet Customisation Excel!")

def customisation_excel():
    """Interface pour la customisation Excel avancÃ©e."""
    
    st.markdown("""
    **Importez un fichier Excel OU utilisez les rÃ©sultats de recherche**
    
    FonctionnalitÃ©s :
    - ðŸ“ Import Excel
    - âœï¸ Ã‰dition en ligne 
    - ðŸ—‘ï¸ Suppression de lignes
    - ðŸ“ Ajout de colonnes d'analyse
    - ðŸ’¾ Export personnalisÃ©
    """)
    
    # Options d'import
    tab1, tab2 = st.tabs(["ðŸ“ Import Fichier", "ðŸ“Š DonnÃ©es de Recherche"])
    
    with tab1:
        # Guide rapide pour l'utilisateur
        with st.expander("ðŸ’¡ Guide: PrÃ©parer votre fichier Excel"):
            st.markdown("""
            **ðŸ“‹ Colonnes recommandÃ©es pour votre fichier Excel:**
            
            **ðŸ”¬ MÃ©tadonnÃ©es principales:**
            - `title` : Titre de l'article
            - `authors` : Auteurs
            - `journal` : Journal/Revue
            - `year` : AnnÃ©e de publication
            - `abstract` : RÃ©sumÃ©
            - `doi` : Identifiant DOI
            
            **ðŸ“Š Colonnes d'analyse (optionnelles):**
            - `ABS 1 OU 0` : PrÃ©sence rÃ©sumÃ© (1=oui, 0=non)
            - `Notes` : Notes personnelles
            - `Type revue` : Type d'Ã©tude
            - `WL = mesure` : Mesures/outcomes
            - `Chr = participants` : CaractÃ©ristiques participants
            - `SpÃ©cialitÃ©` : Domaine mÃ©dical
            - `Intervention` : Type d'intervention
            - `Technique` : Technique utilisÃ©e
            - `Contexte` : Contexte de l'Ã©tude
            - `Simulation ?` : Utilise simulation (Oui/Non)
            - `Outils` : Outils utilisÃ©s
            - `RÃ©sultats ?` : Type de rÃ©sultats
            - `Additional outcomes / measures` : Mesures additionnelles
            - `Exclusion` : Statut inclusion/exclusion
            
            **âœ… L'application dÃ©tecte automatiquement les variations de noms !**
            """)
        
        uploaded_file = st.file_uploader(
            "Choisir un fichier Excel",
            type=['xlsx', 'xls', 'xlsm'],
            help="Formats supportÃ©s: .xlsx, .xls, .xlsm (avec macros)"
        )
        
        if uploaded_file:
            # Afficher les informations du fichier
            file_details = {
                "Nom": uploaded_file.name,
                "Taille": f"{uploaded_file.size / 1024:.1f} KB",
                "Type": uploaded_file.type
            }
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.info(f"ðŸ“„ **{file_details['Nom']}**")
            with col2:
                st.info(f"ðŸ’¾ **{file_details['Taille']}**")
            with col3:
                st.info(f"ðŸ“ **{file_details['Type'].split('.')[-1].upper()}**")
            
            try:
                # Lecture du fichier Excel avec gestion robuste
                df = pd.read_excel(uploaded_file, engine='openpyxl')
                
                # DÃ©tection des colonnes d'analyse existantes avec variations de noms
                analysis_columns = ['ABS 1 OU 0', 'Notes', 'Type revue', 'WL = mesure', 'Chr = participants', 
                                  'SpÃ©cialitÃ©', 'Intervention', 'Technique', 'Contexte', 'Simulation ?', 
                                  'Outils', 'RÃ©sultats ?', 'Additional outcomes / measures', 'Exclusion']
                
                # Normaliser les noms de colonnes pour la dÃ©tection
                df_columns_lower = [col.lower().strip() for col in df.columns]
                analysis_columns_lower = [col.lower().strip() for col in analysis_columns]
                
                # DÃ©tecter les colonnes avec variations de noms
                existing_analysis_cols = []
                column_mapping = {}
                
                for target_col in analysis_columns:
                    target_lower = target_col.lower().strip()
                    
                    # Recherche exacte d'abord
                    if target_col in df.columns:
                        existing_analysis_cols.append(target_col)
                        column_mapping[target_col] = target_col
                    else:
                        # Recherche avec variations (case insensitive, espaces)
                        for df_col in df.columns:
                            df_col_clean = df_col.lower().strip()
                            
                            # VÃ©rifications avec diffÃ©rentes variations
                            if (df_col_clean == target_lower or 
                                df_col_clean.replace(' ', '') == target_lower.replace(' ', '') or
                                df_col_clean.replace('ou', 'OU').replace('?', ' ?') == target_lower):
                                
                                existing_analysis_cols.append(target_col)
                                column_mapping[target_col] = df_col
                                break
                
                # Renommer les colonnes pour standardiser
                if column_mapping:
                    rename_dict = {v: k for k, v in column_mapping.items() if v != k}
                    if rename_dict:
                        df = df.rename(columns=rename_dict)
                        st.info(f"ðŸ”„ {len(rename_dict)} colonne(s) renommÃ©e(s) pour standardisation: {', '.join(rename_dict.keys())}")
                
                missing_analysis_cols = [col for col in analysis_columns if col not in existing_analysis_cols]
                
                # Affichage des informations sur les colonnes
                col1, col2 = st.columns(2)
                with col1:
                    if existing_analysis_cols:
                        st.success(f"âœ… **{len(existing_analysis_cols)} colonnes d'analyse dÃ©tectÃ©es:**")
                        for col in existing_analysis_cols:
                            filled_count = (df[col].astype(str).str.strip() != '').sum()
                            st.caption(f"â€¢ {col}: {filled_count} entrÃ©es")
                    else:
                        st.info("â„¹ï¸ Aucune colonne d'analyse dÃ©tectÃ©e")
                
                with col2:
                    if missing_analysis_cols:
                        st.warning(f"âš ï¸ **{len(missing_analysis_cols)} colonnes d'analyse manquantes:**")
                        st.caption(", ".join(missing_analysis_cols[:3]) + ("..." if len(missing_analysis_cols) > 3 else ""))
                
                # Ajouter automatiquement les colonnes d'analyse si l'option est activÃ©e
                if st.session_state.get('auto_add_columns', False):
                    extended_columns = create_extended_columns()
                    added_count = 0
                    for col_name, default_value in extended_columns.items():
                        if col_name not in df.columns:
                            df[col_name] = default_value
                            added_count += 1
                    
                    # PrÃ©-remplissage automatique
                    if 'abstract' in df.columns and 'ABS 1 OU 0' in df.columns:
                        df['ABS 1 OU 0'] = df['abstract'].apply(
                            lambda x: '1' if pd.notna(x) and str(x).strip() != '' else '0'
                        )
                    
                    if added_count > 0:
                        st.success(f"ðŸš€ {added_count} nouvelles colonnes d'analyse ajoutÃ©es automatiquement!")
                
                st.session_state['excel_data'] = df
                st.success(f"âœ… Fichier importÃ© avec succÃ¨s: {len(df)} lignes, {len(df.columns)} colonnes")
                
                # Afficher quelques colonnes dÃ©tectÃ©es pour confirmation
                basic_cols = ['title', 'authors', 'journal', 'year', 'abstract']
                detected_basic = [col for col in basic_cols if col in df.columns]
                if detected_basic:
                    st.info(f"ðŸ“Š Colonnes principales dÃ©tectÃ©es: {', '.join(detected_basic)}")
                
            except Exception as e:
                error_msg = str(e)
                if "No sheet named" in error_msg:
                    st.error("âŒ Erreur: Impossible de lire la feuille Excel. VÃ©rifiez que le fichier n'est pas corrompu.")
                elif "Unsupported format" in error_msg:
                    st.error("âŒ Erreur: Format de fichier non supportÃ©. Utilisez .xlsx, .xls ou .xlsm")
                elif "Permission denied" in error_msg:
                    st.error("âŒ Erreur: Fichier ouvert dans Excel. Fermez le fichier et rÃ©essayez.")
                else:
                    st.error(f"âŒ Erreur lors de l'import: {error_msg}")
                    st.info("ðŸ’¡ Conseil: Assurez-vous que votre fichier Excel est bien formatÃ© et fermÃ© dans Excel.")
    
    with tab2:
        if 'search_results' in st.session_state:
            if st.button("ðŸ“Š Utiliser rÃ©sultats de recherche"):
                st.session_state['excel_data'] = st.session_state['search_results']
                st.success("âœ… RÃ©sultats de recherche chargÃ©s!")
        else:
            st.info("Effectuez d'abord une recherche dans l'onglet 'Recherche Multi-API'")
    
    # Interface de customisation
    if 'excel_data' in st.session_state:
        df = st.session_state['excel_data'].copy()
        
        st.subheader("ðŸ“ Customisation des DonnÃ©es")
        
        # AperÃ§u des donnÃ©es
        st.write(f"**DonnÃ©es actuelles:** {len(df)} lignes, {len(df.columns)} colonnes")
        
        # Options de customisation
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ðŸ“Š Ajouter colonnes d'analyse", type="secondary"):
                extended_columns = create_extended_columns()
                for col_name, default_value in extended_columns.items():
                    if col_name not in df.columns:
                        df[col_name] = default_value
                
                # PrÃ©-remplissage automatique intelligent
                if 'abstract' in df.columns:
                    df['ABS 1 OU 0'] = df['abstract'].apply(
                        lambda x: '1' if pd.notna(x) and str(x).strip() != '' else '0'
                    )
                
                # Auto-dÃ©tecter le type de revue basÃ© sur le titre
                if 'title' in df.columns:
                    df['Type revue'] = df['title'].apply(detect_review_type)
                
                # Auto-dÃ©tecter la spÃ©cialitÃ© basÃ©e sur le journal
                if 'journal' in df.columns:
                    df['SpÃ©cialitÃ©'] = df['journal'].apply(detect_specialty)
                
                st.session_state['excel_data'] = df
                st.success("âœ… Colonnes d'analyse ajoutÃ©es avec prÃ©-remplissage automatique!")
                st.rerun()
            
            # Bouton pour ajouter automatiquement TOUTES les colonnes Ã  l'import
            if st.button("ðŸš€ Auto-ajouter colonnes au chargement", type="primary"):
                st.session_state['auto_add_columns'] = True
                st.success("âœ… Les colonnes d'analyse seront automatiquement ajoutÃ©es lors du prochain import!")
                st.rerun()
        
        with col2:
            if st.button("ðŸ”„ RÃ©initialiser donnÃ©es"):
                if 'search_results' in st.session_state:
                    st.session_state['excel_data'] = st.session_state['search_results'].copy()
                st.success("âœ… DonnÃ©es rÃ©initialisÃ©es!")
                st.rerun()
        
        with col3:
            show_all_columns = st.checkbox("Afficher toutes les colonnes", value=False)
        
        # SÃ©lection des colonnes Ã  afficher
        if show_all_columns:
            columns_to_show = df.columns.tolist()
        else:
            default_columns = ['title', 'authors', 'journal', 'year', 'source']
            available_defaults = [col for col in default_columns if col in df.columns]
            columns_to_show = st.multiselect(
                "Colonnes Ã  afficher",
                options=df.columns.tolist(),
                default=available_defaults
            )
        
        if columns_to_show:
            display_df = df[columns_to_show].copy()
            
            # Interface de tableau scrollable pour l'Ã©dition
            st.subheader("ðŸ“Š Tableau d'Ã‰dition - Tous les Articles")
            
            # Affichage du nombre total d'articles
            st.info(f"ðŸ“Š **{len(df)} articles** Ã  traiter | âœï¸ Cliquez directement sur les cellules pour les modifier")
            
            # S'assurer que toutes les colonnes d'analyse existent
            analysis_columns = ['ABS 1 OU 0', 'Notes', 'Type revue', 'WL = mesure', 'Chr = participants', 
                              'SpÃ©cialitÃ©', 'Intervention', 'Technique', 'Contexte', 'Simulation ?', 
                              'Outils', 'RÃ©sultats ?', 'Additional outcomes / measures', 'Exclusion']
            
            current_df = st.session_state['excel_data']
            
            # Ajouter les colonnes d'analyse si elles n'existent pas
            for col in analysis_columns:
                if col not in current_df.columns:
                    current_df[col] = ""
            
            # Configuration des colonnes Ã©ditables
            column_config = {}
            
            # Configuration pour les colonnes d'analyse spÃ©cialisÃ©es
            column_config['ABS 1 OU 0'] = st.column_config.SelectboxColumn(
                "ABS 1 OU 0",
                help="1 = RÃ©sumÃ© prÃ©sent, 0 = Pas de rÃ©sumÃ©",
                options=['', '0', '1'],
                default=''
            )
            
            column_config['Simulation ?'] = st.column_config.SelectboxColumn(
                "Simulation ?",
                options=['', 'Oui', 'Non'],
                default=''
            )
            
            column_config['Type revue'] = st.column_config.SelectboxColumn(
                "Type revue",
                options=['', 'Revue systÃ©matique', 'MÃ©ta-analyse', 'ECR', 'Ã‰tude de cohorte', 'Ã‰tude de cas', 'Revue narrative', 'Autre'],
                default=''
            )
            
            column_config['RÃ©sultats ?'] = st.column_config.SelectboxColumn(
                "RÃ©sultats ?",
                options=['', 'Significatifs', 'Non significatifs', 'MitigÃ©s', 'Non rapportÃ©s'],
                default=''
            )
            
            column_config['Exclusion'] = st.column_config.SelectboxColumn(
                "Exclusion",
                options=['', 'Inclus', 'Exclu - CritÃ¨res', 'Exclu - QualitÃ©', 'Exclu - Pertinence', 'Exclu - Doublons'],
                default=''
            )
            
            # Configuration pour les colonnes textuelles
            for col in ['title', 'abstract', 'authors', 'journal', 'Notes', 'WL = mesure', 'Chr = participants', 
                       'SpÃ©cialitÃ©', 'Intervention', 'Technique', 'Contexte', 'Outils', 'Additional outcomes / measures']:
                if col in current_df.columns:
                    column_config[col] = st.column_config.TextColumn(
                        col,
                        width="medium",
                        max_chars=500
                    )
            
            # Configuration pour les colonnes numÃ©riques
            if 'year' in current_df.columns:
                column_config['year'] = st.column_config.NumberColumn(
                    "AnnÃ©e",
                    min_value=1800,
                    max_value=2030,
                    step=1
                )
            
            if 'cited_by' in current_df.columns:
                column_config['cited_by'] = st.column_config.NumberColumn(
                    "Citations",
                    min_value=0,
                    step=1
                )
            
            # Options d'affichage
            col1, col2, col3 = st.columns(3)
            with col1:
                show_analysis_only = st.checkbox("ðŸ“Š Afficher seulement les colonnes d'analyse", value=False)
            with col2:
                show_basic_only = st.checkbox("ðŸ“„ Afficher seulement les mÃ©tadonnÃ©es", value=False)
            with col3:
                compact_view = st.checkbox("ðŸ—œï¸ Vue compacte", value=False)
            
            # Filtrer les colonnes selon le choix
            if show_analysis_only:
                visible_columns = [col for col in current_df.columns if col in analysis_columns]
            elif show_basic_only:
                basic_columns = ['title', 'abstract', 'authors', 'journal', 'year', 'doi', 'cited_by', 'url', 'source', 'type']
                visible_columns = [col for col in current_df.columns if col in basic_columns]
            else:
                # RÃ©organiser les colonnes : mÃ©tadonnÃ©es d'abord, puis analyse
                basic_columns = ['title', 'abstract', 'authors', 'journal', 'year', 'doi', 'cited_by', 'url', 'source', 'type']
                basic_cols_present = [col for col in basic_columns if col in current_df.columns]
                analysis_cols_present = [col for col in analysis_columns if col in current_df.columns]
                other_cols = [col for col in current_df.columns if col not in basic_columns and col not in analysis_columns]
                visible_columns = basic_cols_present + analysis_cols_present + other_cols
            
            # CrÃ©er une vue filtrÃ©e du DataFrame
            df_to_edit = current_df[visible_columns].copy()
            
            # Tableau Ã©ditable avec scrolling
            edited_df = st.data_editor(
                df_to_edit,
                column_config=column_config,
                use_container_width=True,
                height=600 if not compact_view else 400,
                hide_index=False,
                num_rows="dynamic",  # Permet d'ajouter/supprimer des lignes
                key="article_editor"
            )
            
            # Sauvegarder automatiquement les modifications
            if not edited_df.equals(df_to_edit):
                # Mettre Ã  jour le DataFrame principal avec les modifications
                for col in visible_columns:
                    if col in current_df.columns:
                        current_df[col] = edited_df[col]
                
                st.session_state['excel_data'] = current_df.copy()
                
                # Calculer les modifications
                changes_count = 0
                for col in visible_columns:
                    if col in current_df.columns and col in df_to_edit.columns:
                        if not current_df[col].equals(df_to_edit[col]):
                            changes_count += 1
                
                if changes_count > 0:
                    st.success(f"âœ… **{changes_count} colonne(s) modifiÃ©e(s)** - Sauvegarde automatique effectuÃ©e!")
            
            # Statistiques et informations
            st.markdown("---")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ðŸ“„ Total Articles", len(current_df))
            
            with col2:
                analysis_filled = 0
                for col in analysis_columns:
                    if col in current_df.columns:
                        analysis_filled += (current_df[col].astype(str).str.strip() != '').sum()
                st.metric("ðŸ“Š Champs Analyse Remplis", analysis_filled)
            
            with col3:
                total_fields = len(visible_columns) * len(current_df)
                filled_fields = 0
                for col in visible_columns:
                    if col in current_df.columns:
                        filled_fields += (current_df[col].astype(str).str.strip() != '').sum()
                completion_rate = (filled_fields / total_fields * 100) if total_fields > 0 else 0
                st.metric("âœ… Taux Completion", f"{completion_rate:.1f}%")
            
            with col4:
                st.metric("ðŸ” Colonnes Visibles", len(visible_columns))
            
            # Actions globales
            st.markdown("### ðŸ› ï¸ Actions Globales")
            action_col1, action_col2, action_col3 = st.columns(3)
            
            with action_col1:
                if st.button("ðŸ”„ RÃ©initialiser les colonnes d'analyse"):
                    for col in analysis_columns:
                        if col in current_df.columns:
                            current_df[col] = ""
                    st.session_state['excel_data'] = current_df.copy()
                    st.success("âœ… Colonnes d'analyse rÃ©initialisÃ©es!")
                    st.rerun()
            
            with action_col2:
                if st.button("ðŸ§¹ Nettoyer les cellules vides"):
                    for col in current_df.columns:
                        current_df[col] = current_df[col].astype(str).str.strip()
                        current_df[col] = current_df[col].replace('', pd.NA)
                    st.session_state['excel_data'] = current_df.copy()
                    st.success("âœ… Cellules vides nettoyÃ©es!")
                    st.rerun()
            
            with action_col3:
                if st.button("ðŸ“‹ Copier configuration"):
                    st.info("ðŸ’¡ Configuration du tableau sauvegardÃ©e automatiquement!")
            
            # Message d'aide
            with st.expander("ðŸ’¡ Aide - Utilisation du Tableau"):
                st.markdown("""
                **ðŸŽ¯ Comment utiliser ce tableau :**
                
                1. **Ã‰dition directe :** Cliquez sur n'importe quelle cellule pour la modifier
                2. **Navigation :** Utilisez les barres de dÃ©filement horizontale et verticale
                3. **Colonnes spÃ©cialisÃ©es :** Utilisez les listes dÃ©roulantes pour les champs prÃ©dÃ©finis
                4. **Sauvegarde automatique :** Toutes vos modifications sont sauvÃ©es en temps rÃ©el
                5. **Filtres d'affichage :** Utilisez les cases Ã  cocher pour voir seulement certaines colonnes
                6. **Ajout/Suppression :** Utilisez les boutons + et - pour gÃ©rer les lignes
                
                **âœ… Avantages :**
                - Aucune perte de donnÃ©es lors de la navigation
                - Vue d'ensemble complÃ¨te de tous vos articles
                - Modification rapide et intuitive
                - Scrolling infini pour gÃ©rer des milliers d'articles
                """)
        
        # Export final
        st.subheader("ðŸ“¥ Export Final")
        
        # Afficher un rÃ©sumÃ© des colonnes qui seront exportÃ©es
        analysis_columns = ['ABS 1 OU 0', 'Notes', 'Type revue', 'WL = mesure', 'Chr = participants', 
                          'SpÃ©cialitÃ©', 'Intervention', 'Technique', 'Contexte', 'Simulation ?', 
                          'Outils', 'RÃ©sultats ?', 'Additional outcomes / measures', 'Exclusion']
        
        existing_analysis = [col for col in analysis_columns if col in df.columns]
        basic_columns = [col for col in df.columns if col not in analysis_columns]
        
        col1, col2 = st.columns(2)
        with col1:
            st.write("**ðŸ“„ MÃ©tadonnÃ©es incluses:**")
            st.write(f"â€¢ {len(basic_columns)} colonnes de base")
            if len(basic_columns) > 0:
                st.caption(", ".join(basic_columns[:5]) + ("..." if len(basic_columns) > 5 else ""))
        
        with col2:
            st.write("**ðŸ“Š Colonnes d'analyse incluses:**")
            st.write(f"â€¢ {len(existing_analysis)} colonnes d'analyse")
            if len(existing_analysis) > 0:
                st.caption(", ".join(existing_analysis[:3]) + ("..." if len(existing_analysis) > 3 else ""))
        
        if len(existing_analysis) > 0:
            st.success(f"âœ… Toutes vos colonnes d'analyse bibliographique sont incluses dans l'export!")
        else:
            st.info("ðŸ’¡ Ajoutez les colonnes d'analyse avec le bouton 'ðŸ“Š Ajouter colonnes d'analyse'")
        
        export_filename = st.text_input(
            "Nom du fichier Excel final",
            value=f"revue_litterature_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx"
        )
        
        if st.button("ðŸ’¾ GÃ©nÃ©rer Excel PersonnalisÃ©", type="primary"):
            # S'assurer qu'on utilise les donnÃ©es les plus rÃ©centes du session_state
            df_to_export = st.session_state.get('excel_data', df).copy()
            
            # VÃ©rification de debug
            st.info(f"ðŸ” Export de {len(df_to_export)} articles avec {len(df_to_export.columns)} colonnes")
            
            # CrÃ©er le fichier Excel avec multiples feuilles
            output = io.BytesIO()
            
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                # Feuille principale avec TOUTES les colonnes (utiliser df_to_export)
                df_to_export.to_excel(writer, sheet_name='DonnÃ©es CustomisÃ©es', index=False)
                
                # Feuille de statistiques avancÃ©es
                stats_data = {
                    'MÃ©trique': [
                        'Total articles',
                        'Articles avec rÃ©sumÃ© (ABS 1 OU 0 = 1)',
                        'Articles analysÃ©s (Notes remplies)',
                        'Articles inclus (Exclusion = Inclus)',
                        'Articles exclus',
                        'Types de revues identifiÃ©s',
                        'SpÃ©cialitÃ©s identifiÃ©es',
                        'Sources uniques',
                        'Plage d\'annÃ©es',
                        'Citations totales'
                    ],
                    'Valeur': [
                        len(df_to_export),
                        (df_to_export['ABS 1 OU 0'] == '1').sum() if 'ABS 1 OU 0' in df_to_export.columns else 0,
                        (df_to_export['Notes'].str.strip() != '').sum() if 'Notes' in df_to_export.columns else 0,
                        (df_to_export['Exclusion'] == 'Inclus').sum() if 'Exclusion' in df_to_export.columns else 0,
                        (df_to_export['Exclusion'].str.contains('Exclu', na=False)).sum() if 'Exclusion' in df_to_export.columns else 0,
                        df_to_export['Type revue'].nunique() if 'Type revue' in df_to_export.columns else 0,
                        df_to_export['SpÃ©cialitÃ©'].nunique() if 'SpÃ©cialitÃ©' in df_to_export.columns else 0,
                        df_to_export['source'].nunique() if 'source' in df_to_export.columns else 0,
                        f"{df_to_export['year'].min()}-{df_to_export['year'].max()}" if 'year' in df_to_export.columns and df_to_export['year'].notna().any() else "N/A",
                        df_to_export['cited_by'].sum() if 'cited_by' in df_to_export.columns else 0
                    ]
                }
                stats_df = pd.DataFrame(stats_data)
                stats_df.to_excel(writer, sheet_name='Statistiques', index=False)
                
                # Feuille de rÃ©partition par type de revue
                if 'Type revue' in df_to_export.columns:
                    type_counts = df_to_export['Type revue'].value_counts().reset_index()
                    type_counts.columns = ['Type de revue', 'Nombre']
                    type_counts.to_excel(writer, sheet_name='Types de revues', index=False)
                
                # Feuille de rÃ©partition par spÃ©cialitÃ©
                if 'SpÃ©cialitÃ©' in df_to_export.columns:
                    specialty_counts = df_to_export['SpÃ©cialitÃ©'].value_counts().reset_index()
                    specialty_counts.columns = ['SpÃ©cialitÃ©', 'Nombre']
                    specialty_counts.to_excel(writer, sheet_name='SpÃ©cialitÃ©s', index=False)
                
                # Feuille de rÃ©partition par source
                if 'source' in df_to_export.columns:
                    source_counts = df_to_export['source'].value_counts().reset_index()
                    source_counts.columns = ['Source', 'Nombre']
                    source_counts.to_excel(writer, sheet_name='RÃ©partition Sources', index=False)
                
                # Feuille des colonnes d'analyse pour rÃ©fÃ©rence
                analysis_columns = ['ABS 1 OU 0', 'Notes', 'Type revue', 'WL = mesure', 'Chr = participants', 
                                  'SpÃ©cialitÃ©', 'Intervention', 'Technique', 'Contexte', 'Simulation ?', 
                                  'Outils', 'RÃ©sultats ?', 'Additional outcomes / measures', 'Exclusion']
                
                existing_analysis = [col for col in analysis_columns if col in df_to_export.columns]
                if existing_analysis:
                    analysis_info = pd.DataFrame({
                        'Colonne d\'analyse': existing_analysis,
                        'Description': [
                            '1 = RÃ©sumÃ© prÃ©sent, 0 = Pas de rÃ©sumÃ©',
                            'Notes libres sur l\'article',
                            'Type d\'Ã©tude (Revue systÃ©matique, ECR, etc.)',
                            'Mesures de rÃ©sultats (WL = Working List)',
                            'CaractÃ©ristiques des participants',
                            'SpÃ©cialitÃ© mÃ©dicale',
                            'Type d\'intervention Ã©tudiÃ©e',
                            'Technique utilisÃ©e',
                            'Contexte de l\'Ã©tude',
                            'Oui/Non si c\'est une simulation',
                            'Outils ou instruments utilisÃ©s',
                            'RÃ©sultats significatifs ou non',
                            'Mesures de rÃ©sultats additionnelles',
                            'Statut d\'inclusion/exclusion'
                        ][:len(existing_analysis)]
                    })
                    analysis_info.to_excel(writer, sheet_name='LÃ©gende Analyse', index=False)
            
            output.seek(0)
            
            # Bouton de tÃ©lÃ©chargement
            # Validation et statistiques de l'export
            analysis_columns = ['ABS 1 OU 0', 'Notes', 'Type revue', 'WL = mesure', 'Chr = participants', 
                              'SpÃ©cialitÃ©', 'Intervention', 'Technique', 'Contexte', 'Simulation ?', 
                              'Outils', 'RÃ©sultats ?', 'Additional outcomes / measures', 'Exclusion']
            
            filled_analysis_cols = []
            for col in analysis_columns:
                if col in df_to_export.columns:
                    filled_count = (df_to_export[col].astype(str).str.strip() != '').sum()
                    if filled_count > 0:
                        filled_analysis_cols.append(f"{col} ({filled_count})")
            
            st.download_button(
                label="â¬‡ï¸ TÃ©lÃ©charger Excel PersonnalisÃ©",
                data=output.getvalue(),
                file_name=export_filename,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
            st.success("âœ… Fichier Excel personnalisÃ© crÃ©Ã© avec succÃ¨s!")
            
            if filled_analysis_cols:
                st.info(f"ðŸ“Š Colonnes d'analyse avec donnÃ©es: {', '.join(filled_analysis_cols)}")
            else:
                st.warning("âš ï¸ Aucune colonne d'analyse n'a Ã©tÃ© remplie")
            
            # Information dÃ©taillÃ©e sur l'export
            with st.expander("â„¹ï¸ DÃ©tails de l'export"):
                st.write("**Contenu du fichier Excel:**")
                st.write(f"â€¢ **Feuille 'DonnÃ©es CustomisÃ©es'**: {len(df_to_export)} articles avec {len(df_to_export.columns)} colonnes")
                st.write(f"â€¢ **Feuille 'Statistiques'**: MÃ©triques dÃ©taillÃ©es")
                st.write(f"â€¢ **Feuille 'Types de revues'**: RÃ©partition par type d'Ã©tude")
                st.write(f"â€¢ **Feuille 'SpÃ©cialitÃ©s'**: RÃ©partition par domaine mÃ©dical")
                st.write(f"â€¢ **Feuille 'RÃ©partition Sources'**: Origine des articles")
                st.write(f"â€¢ **Feuille 'LÃ©gende Analyse'**: Description des colonnes d'analyse")
                
                if len(filled_analysis_cols) > 0:
                    st.success(f"âœ… {len(filled_analysis_cols)} colonnes d'analyse contiennent des donnÃ©es")
                
                # Afficher un Ã©chantillon des colonnes modifiÃ©es
                st.write("**AperÃ§u des colonnes d'analyse:**")
                # VÃ©rifier quelles colonnes existent vraiment
                available_basic_cols = [col for col in ['title', 'authors', 'journal', 'year'] if col in df_to_export.columns]
                sample_analysis_cols = [col for col in analysis_columns[:5] if col in df_to_export.columns]
                sample_cols = available_basic_cols[:2] + sample_analysis_cols[:3]  # Prendre au max 5 colonnes
                
                if len(df_to_export) > 0 and sample_cols:
                    st.dataframe(df_to_export[sample_cols].head(3), use_container_width=True)
                else:
                    st.info("Aucune donnÃ©e Ã  afficher dans l'aperÃ§u")

def configuration_apis():
    """Interface de configuration des APIs."""
    
    st.markdown("""
    **Configuration et Test des APIs Scientifiques**
    
    Toutes les APIs utilisÃ©es sont **gratuites** et ne nÃ©cessitent pas de clÃ© d'accÃ¨s.
    """)
    
    # Statut des APIs
    st.subheader("ðŸ“¡ Statut des APIs")
    
    for api_name, config in API_CONFIG.items():
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.write(f"**{api_name.title()}**")
        
        with col2:
            st.write("âœ… Gratuite" if config["free"] else "ðŸ’° Payante")
        
        with col3:
            st.write(f"âš¡ {config['rate_limit']}/s" if 'second' in str(config['rate_limit']) else f"âš¡ {config['rate_limit']}/min")
        
        with col4:
            if st.button(f"ðŸ§ª Tester {api_name}", key=f"test_{api_name}"):
                test_api(api_name)
    
    # Informations dÃ©taillÃ©es
    st.subheader("ðŸ“š DÃ©tails des APIs")
    
    with st.expander("ðŸŒ OpenAlex"):
        st.markdown("""
        **OpenAlex** - Base de donnÃ©es bibliographique open source
        - âœ… EntiÃ¨rement gratuite
        - ðŸ“Š 250M+ articles scientifiques
        - ðŸ”„ Mise Ã  jour quotidienne
        - ðŸ·ï¸ MÃ©tadonnÃ©es complÃ¨tes (auteurs, institutions, citations)
        - ðŸŒ Couvre toutes les disciplines
        """)
    
    with st.expander("ðŸ§  Semantic Scholar"):
        st.markdown("""
        **Semantic Scholar** - Moteur de recherche acadÃ©mique avec IA
        - âœ… Gratuite (limite: 100 req/min)
        - ðŸ¤– Analyse sÃ©mantique avancÃ©e
        - ðŸ“ˆ MÃ©triques d'influence
        - ðŸ“„ 200M+ articles
        - ðŸ”— Graphe de citations intelligent
        """)
    
    with st.expander("ðŸ¥ PubMed"):
        st.markdown("""
        **PubMed** - Base biomÃ©dicale officielle NCBI
        - âœ… EntiÃ¨rement gratuite
        - ðŸ¥ Focus biomÃ©dical et vie sciences
        - ðŸ“š 35M+ rÃ©fÃ©rences
        - ðŸ›ï¸ Source officielle gouvernementale
        - ðŸ“‹ Abstracts de qualitÃ© garantie
        """)
    
    with st.expander("ðŸ“„ Crossref"):
        st.markdown("""
        **Crossref** - MÃ©tadonnÃ©es d'articles acadÃ©miques
        - âœ… Gratuite avec politeness policy
        - ðŸ”— 135M+ enregistrements
        - ðŸ“Š MÃ©tadonnÃ©es DOI officielles
        - ðŸ¢ Sources d'Ã©diteurs certifiÃ©s
        - ðŸ“… DonnÃ©es de publication prÃ©cises
        """)

def test_api(api_name: str):
    """Tester une API spÃ©cifique."""
    try:
        if api_name == "openalex":
            api = OpenAlexAPI()
            result = api.search_works("covid", 2023, 2024, 5)
        elif api_name == "semantic_scholar":
            api = SemanticScholarAPI()
            result = api.search_papers("machine learning", 2023, 2024, 5)
        elif api_name == "pubmed":
            api = PubMedAPI()
            result = api.search_articles("cancer therapy", 2023, 2024, 5)
        elif api_name == "crossref":
            api = CrossrefAPI()
            result = api.search_works("artificial intelligence", 2023, 2024, 5)
        
        if not result.empty:
            st.success(f"âœ… {api_name.title()}: {len(result)} articles trouvÃ©s")
            st.dataframe(result[['title', 'authors', 'year']].head(3))
        else:
            st.warning(f"âš ï¸ {api_name.title()}: Test rÃ©ussi mais aucun rÃ©sultat")
    
    except Exception as e:
        st.error(f"âŒ {api_name.title()}: Erreur - {e}")

if __name__ == "__main__":
    main()
